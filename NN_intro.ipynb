{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6a1536-1a6b-4a98-8068-5cf0d21bbb8e",
   "metadata": {},
   "source": [
    "We will explore a neural network model for classification. We will train the model to match images with corresponding numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3eb63-24ce-4d83-b853-1f793eb4082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ec0b83-080e-4eac-887b-d3dc2a0b5e20",
   "metadata": {},
   "source": [
    "The following block of downloading data will only work from the login node i.e when jupyter notebook is launched from the login node on midway3.\n",
    "MNIST is a dataset frequently used in AI training. It may be downloaded via the Pytorch library.\n",
    "\n",
    "The data is split as follows:\n",
    "60,000 training samples and 10,000 test samples.\n",
    "inputs: 1 x 28 x 28 pixels\n",
    "outputs (labels): one integer per image sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e825d7-9268-4838-aeef-9426968da08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08ccc3-8e83-40cb-a21d-b0b465c1c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45c67b-c08b-4373-a6d6-de0274166cbe",
   "metadata": {},
   "source": [
    "Some of the samples in the training data are now separated as validation data. So, now\n",
    "Training data : 48000 samples & validation data = 12000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d08ad-83aa-47ab-ae93-108a1596d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data = torch.utils.data.random_split(training_data, [0.8, 0.2], generator=torch.Generator().manual_seed(55))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3967a08c-c47d-42f7-ba79-3eb30fc086d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MNIST data loaded: train:',len(training_data),' examples, validation: ', len(validation_data), 'examples, test:',len(test_data), 'examples')\n",
    "print('Input shape', training_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ac141-252b-4cf6-8e9c-eeccd1d8c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(numpy.reshape(training_data[i][0], (28, 28)), cmap=\"gray\")\n",
    "    plt.title('Class: '+str(training_data[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a832dc-17f3-4d57-bba0-d62d4d9df3ac",
   "metadata": {},
   "source": [
    "batch_size is the size of data used for each iteration before the model is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f994056-c73b-4bf4-a7b3-4cef132bdb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# The dataloader makes our dataset iterable\n",
    "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
    "val_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c226f35-5095-4b73-a10c-6169010170df",
   "metadata": {},
   "source": [
    "To train our classifier, we need (besides the data):\n",
    "\n",
    "A model that depend on parameters.Here we are going to use neural networks.\n",
    "A loss function to measure the capabilities of the model.\n",
    "An optimization method.\n",
    "\n",
    "The linear layers in PyTorch perform a basic y = xW +b computation. These \"fully connected\" layers connect each input to each output with some weight parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2e018-0129-4e96-bb7b-e891c1b0747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # First, we need to convert the input image to a vector by using\n",
    "        # nn.Flatten(). For MNIST, it means the second dimension 28*28 becomes 784.\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Here, we add a fully connected (\"dense\") layer that has 28 x 28 = 784 input nodes\n",
    "        #(one for each pixel in the input image) and 10 output nodes (for probabilities of each class).\n",
    "        self.layer_1 = nn.Linear(28*28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer_1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c373ecd-ecbb-41ee-9bd3-a3d894550d92",
   "metadata": {},
   "source": [
    "We now need:\n",
    "\n",
    "a) A loss function J,\n",
    " where J is a function of the parameters W and b.\n",
    "\n",
    "b) An optimization method or optimizer such as the stochastic gradient descent (sgd) method (Other choices could be the Adam optimizer, RMSprop, Adagrad).  For far more information about more advanced optimizers than basic SGD, with some cool animations, see https://ruder.io/optimizing-gradient-descent/ or https://distill.pub/2017/momentum/.\n",
    "\n",
    "c) A learning rate (lr). The learning rate controls how far we move during each step or iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6fde1-ff50-4f93-beff-b90ea481f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearClassifier()\n",
    "print(linear_model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(linear_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8256d87e-a836-4a73-9a9a-f1afa5c79546",
   "metadata": {},
   "source": [
    "Let us now train our first model. A training step is comprised of:\n",
    "\n",
    "1) A forward pass: the input is passed through the network\n",
    "2) Backpropagation: A backward pass to compute dJ/dW, i.e the gradient of the loss function with the parameters of the network.\n",
    "3) Weight updates: Wc= W - (lr)* dJ/dW\n",
    " where lr is the learning rate.\n",
    "\n",
    "Things to note:\n",
    "- The batch size corresponds to the number of training examples in one pass (forward + backward). A smaller batch size allows the model to learn from individual examples but takes longer to train. A larger batch size requires fewer steps but may result in the model not capturing the nuances in the data. The higher the batch size, the more memory you will require.\n",
    "\n",
    "- An epoch means one pass through the whole training data (looping over the batches). Using few epochs can lead to underfitting and using too many can lead to overfitting.\n",
    "  \n",
    "- The choice of batch size and learning rate are important for performance, generalization and accuracy in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de87ccce-5668-44db-8c1c-a8a4fa10c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # backward pass calculates gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # take one step with these gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # resets the gradients\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ab28b6-1564-4c4d-9deb-d6e4581d44d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - some NN pieces behave differently during training\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    # We can save computation and memory by not calculating gradients here - we aren't optimizing\n",
    "    with torch.no_grad():\n",
    "        # loop over all of the batches\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            # how many are correct in this batch? Tracking for accuracy\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    accuracy = 100*correct\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2945a705-d7e2-475e-b869-688a1a0fc6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 5\n",
    "for j in range(epochs):\n",
    "    train_one_epoch(train_dataloader, linear_model, loss_fn, optimizer)\n",
    "\n",
    "    # checking on the training loss and accuracy once per epoch\n",
    "    acc, loss = evaluate(train_dataloader, linear_model, loss_fn)\n",
    "    print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07157db5-e808-4d25-86f0-15389001509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how the model is doing on the first 10 examples\n",
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "linear_model.eval()\n",
    "batch = next(iter(train_dataloader))\n",
    "predictions = linear_model(batch[0])\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(batch[0][i,0,:,:], cmap=\"gray\")\n",
    "    plt.title('%d' % predictions[i,:].argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f1bdb-c069-4a17-82ef-3ac1c55cdd99",
   "metadata": {},
   "source": [
    "Now, let's check the model accuracy with the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdce4d6-589a-4d15-a0bc-8a045c00f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_val, loss_val = evaluate(val_dataloader, linear_model, loss_fn)\n",
    "print(\"Validation loss: %.4f, validation accuracy: %.2f%%\" % (loss_val, acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0ef2b-d2f6-420a-b678-484f1724ab67",
   "metadata": {},
   "source": [
    " The following function shows the cases of failure for our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c4b61-4622-4ce8-9a17-6d6e3e30eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_failures(model, dataloader, maxtoshow=100):\n",
    "    model.eval()\n",
    "    batch = next(iter(dataloader))\n",
    "    predictions = model(batch[0])\n",
    "\n",
    "    rounded = predictions.argmax(1)\n",
    "    errors = rounded!=batch[1]\n",
    "    print('Showing max', maxtoshow, 'first failures. '\n",
    "          'The predicted class is shown first and the correct class in parentheses.')\n",
    "    ii = 0\n",
    "    plt.figure(figsize=(maxtoshow, 1))\n",
    "    for i in range(batch[0].shape[0]):\n",
    "        if ii>=maxtoshow:\n",
    "            break\n",
    "        if errors[i]:\n",
    "            plt.subplot(1, maxtoshow, ii+1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(batch[0][i,0,:,:], cmap=\"gray\")\n",
    "            plt.title(\"%d (%d)\" % (rounded[i], batch[1][i]))\n",
    "            ii = ii + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab4bf0e-a632-4091-b99c-59659a14e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_failures(linear_model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1776576-ac4b-43f3-a886-34f78668a5c5",
   "metadata": {},
   "source": [
    "Exercise: How can you improve the accuracy? Some things you might consider: increasing the number of epochs, changing the learning rate, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
